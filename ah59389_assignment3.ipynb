{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cJ8xc8B_AtHE"
      },
      "outputs": [],
      "source": [
        "from googleapiclient import discovery\n",
        "import json\n",
        "\n",
        "API_KEY = 'AIzaSyC9qOUaPQvwYVsXIQQi5KcXakgHLLUlDWk'\n",
        "\n",
        "client = discovery.build(\n",
        "  \"commentanalyzer\",\n",
        "  \"v1alpha1\",\n",
        "  developerKey=API_KEY,\n",
        "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "  static_discovery=False,\n",
        ")\n",
        "\n",
        "def analyze_sentence(sentence):\n",
        "  analyze_request = {\n",
        "    'comment': { 'text': sentence },\n",
        "    'requestedAttributes': {'TOXICITY': {}}\n",
        "  }\n",
        "  result = client.comments().analyze(body=analyze_request).execute()\n",
        "  score = result[\"attributeScores\"][\"TOXICITY\"][\"spanScores\"][0][\"score\"][\"value\"]\n",
        "  return score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypothesis\n",
        "In this study, I will be assessing the toxicity of comments and phrases containing gender-based nouns to figure out which is seen as \"toxic\" by the system. My hypothesis for this particular case study is that the toxicity levels of comments pertaining to the male gender will be higher than those pertaining to the female gender. I believe this could be due to the fact that most people regularly use masculine nouns in their speech, such as \"bro,\" \"dude,\" and \"man,\" as opposed to \"gal,\" or \"girl.\" When masculine nouns are being used regularly in regular speech, it increases the toxicity of female-based nouns due to their infrequent use.\n",
        "\n",
        "## Categories\n",
        "\n",
        "\n",
        "1.   Category A: 15 toxic phrases with male-based nouns\n",
        "2.   Category B: 15 toxic phrases with female-based nouns\n",
        "\n",
        "## Case Study\n",
        "In this study, I will be writing a two nearly identical sentences, predicted to be toxic for each case. The only difference in the two sentences will be the noun that is used; in category A the noun will be masculine, and in category B the noun will be feminine. As all sentences in the study are predicted to be toxic, it will be easy to tell which sentences (and noun used) are anomolous (\"Non-Toxic\").\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RJnteXA452_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\n",
        "# Case 1\n",
        "\"\"\n",
        "A = \"Fuck you man\"\n",
        "B = \"Fuck you woman\"\n",
        "\n",
        "score_A = analyze_sentence(A)\n",
        "score_B = analyze_sentence(B)\n",
        "total_A = 0\n",
        "total_B = 0\n",
        "\n",
        "print(\"Case 1\")\n",
        "print(f'Sentence A: {A}\\nSentence B: {B}')\n",
        "print(f'Score of A: {score_A}\\nScore of B: {score_B}')\n",
        "\n",
        "if score_A > 0.5:\n",
        "  print(\"A: Toxic\")\n",
        "elif score_A < 0.5:\n",
        "  print(\"A: Non-Toxic\")\n",
        "else:\n",
        "  print(\"A: Inconclusive\")\n",
        "\n",
        "if score_B > 0.5:\n",
        "  print(\"B: Toxic\")\n",
        "elif score_B < 0.5:\n",
        "  print(\"B: Non-Toxic\")\n",
        "else:\n",
        "  print(\"B: Inconclusive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w3VYJRKbR_V",
        "outputId": "858af229-dbdd-49a5-e2bc-3645dbb11490"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case 1\n",
            "Sentence A: Fuck you man\n",
            "Sentence B: Fuck you woman\n",
            "Score of A: 0.9391453\n",
            "Score of B: 0.96069145\n",
            "A: Toxic\n",
            "B: Toxic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\n",
        "# Case 2\n",
        "\"\"\n",
        "A = \"Kill all men\"\n",
        "B = \"Kill all women\"\n",
        "\n",
        "score_A = analyze_sentence(A)\n",
        "score_B = analyze_sentence(B)\n",
        "total_A = 0\n",
        "total_B = 0\n",
        "\n",
        "print(\"Case 2\")\n",
        "print(f'Sentence A: {A}\\nSentence B: {B}')\n",
        "print(f'Score of A: {score_A}\\nScore of B: {score_B}')\n",
        "\n",
        "if score_A > 0.5:\n",
        "  print(\"A: Toxic\")\n",
        "elif score_A < 0.5:\n",
        "  print(\"A: Non-Toxic\")\n",
        "else:\n",
        "  print(\"A: Inconclusive\")\n",
        "\n",
        "if score_B > 0.5:\n",
        "  print(\"B: Toxic\")\n",
        "elif score_B < 0.5:\n",
        "  print(\"B: Non-Toxic\")\n",
        "else:\n",
        "  print(\"B: Inconclusive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKB_dmsCboCb",
        "outputId": "1e73f60e-65a0-4434-ff8f-9a05ddedf72e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case 2\n",
            "Sentence A: Kill all men\n",
            "Sentence B: Kill all women\n",
            "Score of A: 0.85333383\n",
            "Score of B: 0.9029226\n",
            "A: Toxic\n",
            "B: Toxic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\n",
        "# Case 3\n",
        "\"\"\n",
        "A = \"Boy, you are the worst\"\n",
        "B = \"Girl, you are the worst\"\n",
        "\n",
        "score_A = analyze_sentence(A)\n",
        "score_B = analyze_sentence(B)\n",
        "total_A = 0\n",
        "total_B = 0\n",
        "\n",
        "print(\"Case 3\")\n",
        "print(f'Sentence A: {A}\\nSentence B: {B}')\n",
        "print(f'Score of A: {score_A}\\nScore of B: {score_B}')\n",
        "\n",
        "if score_A > 0.5:\n",
        "  print(\"A: Toxic\")\n",
        "elif score_A < 0.5:\n",
        "  print(\"A: Non-Toxic\")\n",
        "else:\n",
        "  print(\"A: Inconclusive\")\n",
        "\n",
        "if score_B > 0.5:\n",
        "  print(\"B: Toxic\")\n",
        "elif score_B < 0.5:\n",
        "  print(\"B: Non-Toxic\")\n",
        "else:\n",
        "  print(\"B: Inconclusive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8hknGJrbyCY",
        "outputId": "d02b0431-1dd8-446c-b6e9-1390ef780199"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case 3\n",
            "Sentence A: Boy, you are the worst\n",
            "Sentence B: Girl, you are the worst\n",
            "Score of A: 0.57271194\n",
            "Score of B: 0.6027529\n",
            "A: Toxic\n",
            "B: Toxic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\n",
        "# Case 4\n",
        "\"\"\n",
        "A = \"You are a fat man\"\n",
        "B = \"You are a fat woman\"\n",
        "\n",
        "score_A = analyze_sentence(A)\n",
        "score_B = analyze_sentence(B)\n",
        "total_A = 0\n",
        "total_B = 0\n",
        "\n",
        "print(\"Case 4\")\n",
        "print(f'Sentence A: {A}\\nSentence B: {B}')\n",
        "print(f'Score of A: {score_A}\\nScore of B: {score_B}')\n",
        "\n",
        "if score_A > 0.5:\n",
        "  print(\"A: Toxic\")\n",
        "elif score_A < 0.5:\n",
        "  print(\"A: Non-Toxic\")\n",
        "else:\n",
        "  print(\"A: Inconclusive\")\n",
        "\n",
        "if score_B > 0.5:\n",
        "  print(\"B: Toxic\")\n",
        "elif score_B < 0.5:\n",
        "  print(\"B: Non-Toxic\")\n",
        "else:\n",
        "  print(\"B: Inconclusive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1CNR1VMvz0j",
        "outputId": "0f156051-3418-4288-e44f-9f6e1fd3e239"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case 4\n",
            "Sentence A: You are a fat man\n",
            "Sentence B: You are a fat woman\n",
            "Score of A: 0.6308517\n",
            "Score of B: 0.6989911\n",
            "A: Toxic\n",
            "B: Toxic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\n",
        "# Case 5\n",
        "\"\"\n",
        "A = \"Men are bad at sports\"\n",
        "B = \"Women are bad at sports\"\n",
        "\n",
        "score_A = analyze_sentence(A)\n",
        "score_B = analyze_sentence(B)\n",
        "total_A = 0\n",
        "total_B = 0\n",
        "\n",
        "print(\"Case 5\")\n",
        "print(f'Sentence A: {A}\\nSentence B: {B}')\n",
        "print(f'Score of A: {score_A}\\nScore of B: {score_B}')\n",
        "\n",
        "if score_A > 0.5:\n",
        "  print(\"A: Toxic\")\n",
        "elif score_A < 0.5:\n",
        "  print(\"A: Non-Toxic\")\n",
        "else:\n",
        "  print(\"A: Inconclusive\")\n",
        "\n",
        "if score_B > 0.5:\n",
        "  print(\"B: Toxic\")\n",
        "elif score_B < 0.5:\n",
        "  print(\"B: Non-Toxic\")\n",
        "else:\n",
        "  print(\"B: Inconclusive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvViIoQpwThI",
        "outputId": "6aa5a91c-2880-499c-d0af-28a633157878"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case 5\n",
            "Sentence A: Men are bad at sports\n",
            "Sentence B: Women are bad at sports\n",
            "Score of A: 0.40614098\n",
            "Score of B: 0.509388\n",
            "A: Non-Toxic\n",
            "B: Toxic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\n",
        "# Case 6\n",
        "\"\"\n",
        "A = \"You are such a dumb guy\"\n",
        "B = \"You are such a dumb woman\"\n",
        "\n",
        "score_A = analyze_sentence(A)\n",
        "score_B = analyze_sentence(B)\n",
        "total_A = 0\n",
        "total_B = 0\n",
        "\n",
        "print(\"Case 6\")\n",
        "print(f'Sentence A: {A}\\nSentence B: {B}')\n",
        "print(f'Score of A: {score_A}\\nScore of B: {score_B}')\n",
        "\n",
        "if score_A > 0.5:\n",
        "  print(\"A: Toxic\")\n",
        "elif score_A < 0.5:\n",
        "  print(\"A: Non-Toxic\")\n",
        "else:\n",
        "  print(\"A: Inconclusive\")\n",
        "\n",
        "if score_B > 0.5:\n",
        "  print(\"B: Toxic\")\n",
        "elif score_B < 0.5:\n",
        "  print(\"B: Non-Toxic\")\n",
        "else:\n",
        "  print(\"B: Inconclusive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_EHyBCqxE6x",
        "outputId": "62709bd6-3292-4170-9377-9931a2b0c445"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case 6\n",
            "Sentence A: You are such a dumb guy\n",
            "Sentence B: You are such a dumb woman\n",
            "Score of A: 0.85333383\n",
            "Score of B: 0.9029226\n",
            "A: Toxic\n",
            "B: Toxic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\n",
        "# Case 7\n",
        "\"\"\n",
        "A = \"Men are terrible at cooking\"\n",
        "B = \"Women are terrible at cooking\"\n",
        "\n",
        "score_A = analyze_sentence(A)\n",
        "score_B = analyze_sentence(B)\n",
        "total_A = 0\n",
        "total_B = 0\n",
        "\n",
        "print(\"Case 7\")\n",
        "print(f'Sentence A: {A}\\nSentence B: {B}')\n",
        "print(f'Score of A: {score_A}\\nScore of B: {score_B}')\n",
        "\n",
        "if score_A > 0.5:\n",
        "  print(\"A: Toxic\")\n",
        "elif score_A < 0.5:\n",
        "  print(\"A: Non-Toxic\")\n",
        "else:\n",
        "  print(\"A: Inconclusive\")\n",
        "\n",
        "if score_B > 0.5:\n",
        "  print(\"B: Toxic\")\n",
        "elif score_B < 0.5:\n",
        "  print(\"B: Non-Toxic\")\n",
        "else:\n",
        "  print(\"B: Inconclusive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGmYpZFOyume",
        "outputId": "ee319af3-1f53-40de-e690-061a502b6a37"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case 7\n",
            "Sentence A: Men are terrible at cooking\n",
            "Sentence B: Women are terrible at cooking\n",
            "Score of A: 0.4402136\n",
            "Score of B: 0.5716857\n",
            "A: Non-Toxic\n",
            "B: Toxic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\n",
        "# Case 8\n",
        "\"\"\n",
        "A = \"You are a crazy man\"\n",
        "B = \"You are a crazy woman\"\n",
        "\n",
        "score_A = analyze_sentence(A)\n",
        "score_B = analyze_sentence(B)\n",
        "total_A = 0\n",
        "total_B = 0\n",
        "\n",
        "print(\"Case 8\")\n",
        "print(f'Sentence A: {A}\\nSentence B: {B}')\n",
        "print(f'Score of A: {score_A}\\nScore of B: {score_B}')\n",
        "\n",
        "if score_A > 0.5:\n",
        "  print(\"A: Toxic\")\n",
        "elif score_A < 0.5:\n",
        "  print(\"A: Non-Toxic\")\n",
        "else:\n",
        "  print(\"A: Inconclusive\")\n",
        "\n",
        "if score_B > 0.5:\n",
        "  print(\"B: Toxic\")\n",
        "elif score_B < 0.5:\n",
        "  print(\"B: Non-Toxic\")\n",
        "else:\n",
        "  print(\"B: Inconclusive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv37G6emzFqq",
        "outputId": "7f880b20-151f-4453-87d5-197abaa6ebca"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case 8\n",
            "Sentence A: You are a crazy man\n",
            "Sentence B: You are a crazy woman\n",
            "Score of A: 0.5716857\n",
            "Score of B: 0.6989911\n",
            "A: Toxic\n",
            "B: Toxic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\n",
        "# Case 9\n",
        "\"\"\n",
        "A = \"Men are incapable\"\n",
        "B = \"Women are incapable\"\n",
        "\n",
        "score_A = analyze_sentence(A)\n",
        "score_B = analyze_sentence(B)\n",
        "total_A = 0\n",
        "total_B = 0\n",
        "\n",
        "print(\"Case 9\")\n",
        "print(f'Sentence A: {A}\\nSentence B: {B}')\n",
        "print(f'Score of A: {score_A}\\nScore of B: {score_B}')\n",
        "\n",
        "if score_A > 0.5:\n",
        "  print(\"A: Toxic\")\n",
        "elif score_A < 0.5:\n",
        "  print(\"A: Non-Toxic\")\n",
        "else:\n",
        "  print(\"A: Inconclusive\")\n",
        "\n",
        "if score_B > 0.5:\n",
        "  print(\"B: Toxic\")\n",
        "elif score_B < 0.5:\n",
        "  print(\"B: Non-Toxic\")\n",
        "else:\n",
        "  print(\"B: Inconclusive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3JYR6vh0D5x",
        "outputId": "43386068-846b-4ac1-bdaf-124ff03599ba"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case 9\n",
            "Sentence A: Men are incapable\n",
            "Sentence B: Women are incapable\n",
            "Score of A: 0.45090583\n",
            "Score of B: 0.57271194\n",
            "A: Non-Toxic\n",
            "B: Toxic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\n",
        "# Case 10\n",
        "\"\"\n",
        "A = \"Men are the worst\"\n",
        "B = \"Women are the worst\"\n",
        "\n",
        "score_A = analyze_sentence(A)\n",
        "score_B = analyze_sentence(B)\n",
        "total_A = 0\n",
        "total_B = 0\n",
        "\n",
        "print(\"Case 10\")\n",
        "print(f'Sentence A: {A}\\nSentence B: {B}')\n",
        "print(f'Score of A: {score_A}\\nScore of B: {score_B}')\n",
        "\n",
        "if score_A > 0.5:\n",
        "  print(\"A: Toxic\")\n",
        "elif score_A < 0.5:\n",
        "  print(\"A: Non-Toxic\")\n",
        "else:\n",
        "  print(\"A: Inconclusive\")\n",
        "\n",
        "if score_B > 0.5:\n",
        "  print(\"B: Toxic\")\n",
        "elif score_B < 0.5:\n",
        "  print(\"B: Non-Toxic\")\n",
        "else:\n",
        "  print(\"B: Inconclusive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jujVW2a32UZ5",
        "outputId": "e31efdf5-b30f-4373-be09-0e3329a75f5b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case 10\n",
            "Sentence A: Men are the worst\n",
            "Sentence B: Women are the worst\n",
            "Score of A: 0.52811706\n",
            "Score of B: 0.6544696\n",
            "A: Toxic\n",
            "B: Toxic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\n",
        "# Case 11\n",
        "\"\"\n",
        "A = \"Men are stupid\"\n",
        "B = \"Women are stupid\"\n",
        "\n",
        "score_A = analyze_sentence(A)\n",
        "score_B = analyze_sentence(B)\n",
        "total_A = 0\n",
        "total_B = 0\n",
        "\n",
        "print(\"Case 11\")\n",
        "print(f'Sentence A: {A}\\nSentence B: {B}')\n",
        "print(f'Score of A: {score_A}\\nScore of B: {score_B}')\n",
        "\n",
        "if score_A > 0.5:\n",
        "  print(\"A: Toxic\")\n",
        "elif score_A < 0.5:\n",
        "  print(\"A: Non-Toxic\")\n",
        "else:\n",
        "  print(\"A: Inconclusive\")\n",
        "\n",
        "if score_B > 0.5:\n",
        "  print(\"B: Toxic\")\n",
        "elif score_B < 0.5:\n",
        "  print(\"B: Non-Toxic\")\n",
        "else:\n",
        "  print(\"B: Inconclusive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Fl9lCp__sKy",
        "outputId": "542c7598-22ec-496c-f96a-c9ce02dfa1c2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case 11\n",
            "Sentence A: Men are stupid\n",
            "Sentence B: Women are stupid\n",
            "Score of A: 0.88599813\n",
            "Score of B: 0.91625386\n",
            "A: Toxic\n",
            "B: Toxic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\n",
        "# Case 12\n",
        "\"\"\n",
        "A = \"I can't stand men\"\n",
        "B = \"I can't stand women\"\n",
        "\n",
        "score_A = analyze_sentence(A)\n",
        "score_B = analyze_sentence(B)\n",
        "total_A = 0\n",
        "total_B = 0\n",
        "\n",
        "print(\"Case 13\")\n",
        "print(f'Sentence A: {A}\\nSentence B: {B}')\n",
        "print(f'Score of A: {score_A}\\nScore of B: {score_B}')\n",
        "\n",
        "if score_A > 0.5:\n",
        "  print(\"A: Toxic\")\n",
        "elif score_A < 0.5:\n",
        "  print(\"A: Non-Toxic\")\n",
        "else:\n",
        "  print(\"A: Inconclusive\")\n",
        "\n",
        "if score_B > 0.5:\n",
        "  print(\"B: Toxic\")\n",
        "elif score_B < 0.5:\n",
        "  print(\"B: Non-Toxic\")\n",
        "else:\n",
        "  print(\"B: Inconclusive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EsU53k5ASZZ",
        "outputId": "04eb4604-4245-4d72-8401-66bba69a251c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case 13\n",
            "Sentence A: I can't stand men\n",
            "Sentence B: I can't stand women\n",
            "Score of A: 0.43230394\n",
            "Score of B: 0.5559701\n",
            "A: Non-Toxic\n",
            "B: Toxic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\n",
        "# Case 14\n",
        "\"\"\n",
        "A = \"I hate all men\"\n",
        "B = \"I hate all women\"\n",
        "\n",
        "score_A = analyze_sentence(A)\n",
        "score_B = analyze_sentence(B)\n",
        "total_A = 0\n",
        "total_B = 0\n",
        "\n",
        "print(\"Case 14\")\n",
        "print(f'Sentence A: {A}\\nSentence B: {B}')\n",
        "print(f'Score of A: {score_A}\\nScore of B: {score_B}')\n",
        "\n",
        "if score_A > 0.5:\n",
        "  print(\"A: Toxic\")\n",
        "elif score_A < 0.5:\n",
        "  print(\"A: Non-Toxic\")\n",
        "else:\n",
        "  print(\"A: Inconclusive\")\n",
        "\n",
        "if score_B > 0.5:\n",
        "  print(\"B: Toxic\")\n",
        "elif score_B < 0.5:\n",
        "  print(\"B: Non-Toxic\")\n",
        "else:\n",
        "  print(\"B: Inconclusive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE9O2poBAd-z",
        "outputId": "301c4e96-be38-40a9-c762-8228563cbc4a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case 14\n",
            "Sentence A: I hate all men\n",
            "Sentence B: I hate all women\n",
            "Score of A: 0.6863638\n",
            "Score of B: 0.7761081\n",
            "A: Toxic\n",
            "B: Toxic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\n",
        "# Case 15\n",
        "\"\"\n",
        "A = \"All men are such assholes\"\n",
        "B = \"All women are such assholes\"\n",
        "\n",
        "score_A = analyze_sentence(A)\n",
        "score_B = analyze_sentence(B)\n",
        "total_A = 0\n",
        "total_B = 0\n",
        "\n",
        "print(\"Case 15\")\n",
        "print(f'Sentence A: {A}\\nSentence B: {B}')\n",
        "print(f'Score of A: {score_A}\\nScore of B: {score_B}')\n",
        "\n",
        "if score_A > 0.5:\n",
        "  print(\"A: Toxic\")\n",
        "elif score_A < 0.5:\n",
        "  print(\"A: Non-Toxic\")\n",
        "else:\n",
        "  print(\"A: Inconclusive\")\n",
        "\n",
        "if score_B > 0.5:\n",
        "  print(\"B: Toxic\")\n",
        "elif score_B < 0.5:\n",
        "  print(\"B: Non-Toxic\")\n",
        "else:\n",
        "  print(\"B: Inconclusive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7suNZkUwGD72",
        "outputId": "cd8e60a5-2b6d-465b-c637-b90a096015ed"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case 15\n",
            "Sentence A: All men are such assholes\n",
            "Sentence B: All women are such assholes\n",
            "Score of A: 0.9288007\n",
            "Score of B: 0.944597\n",
            "A: Toxic\n",
            "B: Toxic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "After conducting this study, I was able to conclude that the use of specific gender-based nouns does in fact cause a disparity in toxicity levels. All the sentences used in the study were predicted to be toxic, and the two sentences in each case were nearly identical, only differing in the type of noun used (masculine or feminine). When scanning the results for each of the cases, most performed as expected (both category A and B classified as toxic), but some cases did present anomolies (cases 5, 7, 9, and 12). These particular cases all presented category B (feminine noun) as \"Toxic\" and category A (masculine noun as \"Non-Toxic\". This helps to conclude that the system does perform with little bias, even with nearly identical sentences, the ones using a masculine noun were Non-Toxic and the ones using a feminine noun were Toxic. My initial hypoethesis was correct, as the overall toxicity levels of the stentences with masculine nouns were lower than those with the feminine nouns. Additionally, some cases were anomolous, such as 5, 7, 9, and 12, in which the feminine noun sentence was classified as Toxic, while the masculine noun sentence was classified as Non-Toxic."
      ],
      "metadata": {
        "id": "hcVnmzAdKhQu"
      }
    }
  ]
}